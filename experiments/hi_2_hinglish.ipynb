{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = Path(Path().resolve().parent, \"models\", \"whisper\")\n",
    "\n",
    "# Define model names\n",
    "MODEL_NAMES = [\"tiny\", \"tiny.en\", \"small\", \"small.en\", \"base\", \"base.en\", \"medium\", \"medium.en\", \"large\", \"large-v2\", \"large-v3\"]\n",
    "\n",
    "# Create paths and check if they exist\n",
    "MODEL_PATHS = {name.replace('.', '_'): Path(BASE_PATH, f\"whisper-{name}\") for name in MODEL_NAMES}\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Path {path} not found. Please verify if the model was correctly downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for model: tiny\n",
      "Tokenizer for model tiny has 51865 tokens.\n",
      "\n",
      "Loading tokenizer for model: tiny_en\n",
      "Tokenizer for model tiny_en has 51864 tokens.\n",
      "\n",
      "Loading tokenizer for model: small\n",
      "Tokenizer for model small has 51865 tokens.\n",
      "\n",
      "Loading tokenizer for model: small_en\n",
      "Tokenizer for model small_en has 51864 tokens.\n",
      "\n",
      "Loading tokenizer for model: base\n",
      "Tokenizer for model base has 51865 tokens.\n",
      "\n",
      "Loading tokenizer for model: base_en\n",
      "Tokenizer for model base_en has 51864 tokens.\n",
      "\n",
      "Loading tokenizer for model: medium\n",
      "Tokenizer for model medium has 50364 tokens.\n",
      "\n",
      "Loading tokenizer for model: medium_en\n",
      "Tokenizer for model medium_en has 51864 tokens.\n",
      "\n",
      "Loading tokenizer for model: large\n",
      "Tokenizer for model large has 51865 tokens.\n",
      "\n",
      "Loading tokenizer for model: large-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for model large-v2 has 51865 tokens.\n",
      "\n",
      "Loading tokenizer for model: large-v3\n",
      "Tokenizer for model large-v3 has 51866 tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "# Load tokenizers\n",
    "TOKENIZERS = {}\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    print(f\"Loading tokenizer for model: {name}\")\n",
    "    TOKENIZERS[name] = WhisperTokenizer.from_pretrained(path)\n",
    "    print(f\"Tokenizer for model {name} has {len(TOKENIZERS[name].get_vocab())} tokens.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def tokenize_texts(texts: dict) -> None:\n",
    "    \"\"\"\n",
    "    Tokenize multiple texts from all tokenizers and print lengths side by side.\n",
    "    \n",
    "    Args:\n",
    "    texts (dict): A dictionary with keys as text descriptions and values as the texts.\n",
    "    \"\"\"\n",
    "    headers = [\"Model\"] + list(texts.keys())\n",
    "    table = []\n",
    "\n",
    "    for name, tokenizer in TOKENIZERS.items():\n",
    "        row = [name]\n",
    "        for text in texts.values():\n",
    "            tokens = tokenizer(text)['input_ids']\n",
    "            row.append(len(tokens))\n",
    "        table.append(row)\n",
    "\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+----------+\n",
      "|   Model   | Hindi | English | Hinglish |\n",
      "+-----------+-------+---------+----------+\n",
      "|   tiny    |  275  |   65    |   126    |\n",
      "|  tiny_en  |  398  |   59    |   137    |\n",
      "|   small   |  275  |   65    |   126    |\n",
      "| small_en  |  398  |   59    |   137    |\n",
      "|   base    |  275  |   65    |   126    |\n",
      "|  base_en  |  398  |   59    |   137    |\n",
      "|  medium   |  275  |   65    |   126    |\n",
      "| medium_en |  398  |   59    |   137    |\n",
      "|   large   |  275  |   65    |   126    |\n",
      "| large-v2  |  275  |   65    |   126    |\n",
      "| large-v3  |  275  |   59    |   126    |\n",
      "+-----------+-------+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "hi_text = \"यह किसी लेख, निबंध या रचना का अंश भी हो सकता है किन्तु स्वयं में पूर्ण होना चाहिए। किसी भी शब्द, वाक्य, सूत्र से सम्बद्ध विचार एवं भावों अपने अर्जित ज्ञान, निजी अनुभूति से संजोकर प्रवाहमयी शैली के माध्यम से गद्यभाषा में अभिव्यक्त करना अनुच्छेद कहलाता है।\"\n",
    "\n",
    "hinglish_text = \"yah kisee lekh, nibandh ya rachana ka ansh bhee ho sakata hai kintu svayan mein poorn hona chaahie. kisee bhee shabd, vaaky, sootr se sambaddh vichaar evan bhaavon ko apane arjit gyaan, nijee anubhooti se sanjokar pravaahamayee shailee ke maadhyam se gadyabhaasha mein abhivyakt karana anuchchhed kahalaata hai.\"\n",
    "\n",
    "en_text = \"It can be a part of any article, essay or composition but it should be complete in itself. Expressing the thoughts and feelings related to any word, sentence, formula in prose language through a flowing style by collecting them with your acquired knowledge and personal experience is called a paragraph.\"\n",
    "\n",
    "texts = {\n",
    "    \"Hindi\": hi_text,\n",
    "    \"English\": en_text,\n",
    "    \"Hinglish\": hinglish_text\n",
    "}\n",
    "\n",
    "tokenize_texts(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftranscript|>', '<|notimestamps|>', 'à¤', '¯', 'à¤', '¹', 'Ġà¤', 'ķ', 'à¤', '¿', 'à¤', '¸', 'à¥', 'Ģ', 'Ġà¤', '²', 'à¥', 'ĩ', 'à¤', 'ĸ', ',', 'Ġà¤', '¨', 'à¤', '¿', 'à¤', '¬', 'à¤', 'Ĥ', 'à¤', '§', 'Ġà¤', '¯', 'à¤¾', 'Ġà¤', '°', 'à¤', 'ļ', 'à¤', '¨', 'à¤¾', 'Ġà¤', 'ķ', 'à¤¾', 'Ġà¤', 'ħ', 'à¤', 'Ĥ', 'à¤', '¶', 'Ġà¤', 'Ń', 'à¥', 'Ģ', 'Ġà¤', '¹', 'à¥', 'ĭ', 'Ġà¤', '¸', 'à¤', 'ķ', 'à¤', '¤', 'à¤¾', 'Ġà¤', '¹', 'à¥', 'Ī', 'Ġà¤', 'ķ', 'à¤', '¿', 'à¤', '¨', 'à¥', 'į', 'à¤', '¤', 'à¥', 'ģ', 'Ġà¤', '¸', 'à¥', 'į', 'à¤', 'µ', 'à¤', '¯', 'à¤', 'Ĥ', 'Ġà¤', '®', 'à¥', 'ĩ', 'à¤', 'Ĥ', 'Ġà¤', 'ª', 'à¥', 'Ĥ', 'à¤', '°', 'à¥', 'į', 'à¤', '£', 'Ġà¤', '¹', 'à¥', 'ĭ', 'à¤', '¨', 'à¤¾', 'Ġà¤', 'ļ', 'à¤¾', 'à¤', '¹', 'à¤', '¿', 'à¤', 'ı', 'à¥', '¤', 'Ġà¤', 'ķ', 'à¤', '¿', 'à¤', '¸', 'à¥', 'Ģ', 'Ġà¤', 'Ń', 'à¥', 'Ģ', 'Ġà¤', '¶', 'à¤', '¬', 'à¥', 'į', 'à¤', '¦', ',', 'Ġà¤', 'µ', 'à¤¾', 'à¤', 'ķ', 'à¥', 'į', 'à¤', '¯', ',', 'Ġà¤', '¸', 'à¥', 'Ĥ', 'à¤', '¤', 'à¥', 'į', 'à¤', '°', 'Ġà¤', '¸', 'à¥', 'ĩ', 'Ġà¤', '¸', 'à¤', '®', 'à¥', 'į', 'à¤', '¬', 'à¤', '¦', 'à¥', 'į', 'à¤', '§', 'Ġà¤', 'µ', 'à¤', '¿', 'à¤', 'ļ', 'à¤¾', 'à¤', '°', 'Ġà¤', 'ı', 'à¤', 'µ', 'à¤', 'Ĥ', 'Ġà¤', 'Ń', 'à¤¾', 'à¤', 'µ', 'à¥', 'ĭ', 'à¤', 'Ĥ', 'Ġà¤', 'ķ', 'à¥', 'ĭ', 'Ġà¤', 'ħ', 'à¤', 'ª', 'à¤', '¨', 'à¥', 'ĩ', 'Ġà¤', 'ħ', 'à¤', '°', 'à¥', 'į', 'à¤', 'ľ', 'à¤', '¿', 'à¤', '¤', 'Ġà¤', 'ľ', 'à¥', 'į', 'à¤', 'ŀ', 'à¤¾', 'à¤', '¨', ',', 'Ġà¤', '¨', 'à¤', '¿', 'à¤', 'ľ', 'à¥', 'Ģ', 'Ġà¤', 'ħ', 'à¤', '¨', 'à¥', 'ģ', 'à¤', 'Ń', 'à¥', 'Ĥ', 'à¤', '¤', 'à¤', '¿', 'Ġà¤', '¸', 'à¥', 'ĩ', 'Ġà¤', '¸', 'à¤', 'Ĥ', 'à¤', 'ľ', 'à¥', 'ĭ', 'à¤', 'ķ', 'à¤', '°', 'Ġà¤', 'ª', 'à¥', 'į', 'à¤', '°', 'à¤', 'µ', 'à¤¾', 'à¤', '¹', 'à¤', '®', 'à¤', '¯', 'à¥', 'Ģ', 'Ġà¤', '¶', 'à¥', 'Ī', 'à¤', '²', 'à¥', 'Ģ', 'Ġà¤', 'ķ', 'à¥', 'ĩ', 'Ġà¤', '®', 'à¤¾', 'à¤', '§', 'à¥', 'į', 'à¤', '¯', 'à¤', '®', 'Ġà¤', '¸', 'à¥', 'ĩ', 'Ġà¤', 'Ĺ', 'à¤', '¦', 'à¥', 'į', 'à¤', '¯', 'à¤', 'Ń', 'à¤¾', 'à¤', '·', 'à¤¾', 'Ġà¤', '®', 'à¥', 'ĩ', 'à¤', 'Ĥ', 'Ġà¤', 'ħ', 'à¤', 'Ń', 'à¤', '¿', 'à¤', 'µ', 'à¥', 'į', 'à¤', '¯', 'à¤', 'ķ', 'à¥', 'į', 'à¤', '¤', 'Ġà¤', 'ķ', 'à¤', '°', 'à¤', '¨', 'à¤¾', 'Ġà¤', 'ħ', 'à¤', '¨', 'à¥', 'ģ', 'à¤', 'ļ', 'à¥', 'į', 'à¤', 'Ľ', 'à¥', 'ĩ', 'à¤', '¦', 'Ġà¤', 'ķ', 'à¤', '¹', 'à¤', '²', 'à¤¾', 'à¤', '¤', 'à¤¾', 'Ġà¤', '¹', 'à¥', 'Ī', 'à¥', '¤', '<|endoftext|>']\n",
      "\n",
      "['<|startoftranscript|>', '<|notimestamps|>', 'à¤¯', 'à¤¹', 'Ġà¤ķ', 'à¤¿', 'à¤¸', 'à¥Ģ', 'Ġà¤', '²', 'à¥ĩ', 'à¤', 'ĸ', ',', 'Ġà¤', '¨', 'à¤¿', 'à¤', '¬', 'à¤Ĥ', 'à¤', '§', 'Ġà¤', '¯', 'à¤¾', 'Ġà¤', '°', 'à¤', 'ļ', 'à¤¨', 'à¤¾', 'Ġà¤ķ', 'à¤¾', 'Ġà¤', 'ħ', 'à¤Ĥ', 'à¤', '¶', 'Ġà¤', 'Ń', 'à¥Ģ', 'Ġà¤¹', 'à¥ĭ', 'Ġà¤¸', 'à¤ķ', 'à¤¤', 'à¤¾', 'Ġà¤¹', 'à¥Ī', 'Ġà¤ķ', 'à¤¿', 'à¤¨', 'à¥į', 'à¤¤', 'à¥', 'ģ', 'Ġà¤¸', 'à¥į', 'à¤', 'µ', 'à¤¯', 'à¤Ĥ', 'Ġà¤®', 'à¥ĩ', 'à¤Ĥ', 'Ġà¤', 'ª', 'à¥', 'Ĥ', 'à¤°', 'à¥į', 'à¤', '£', 'Ġà¤¹', 'à¥ĭ', 'à¤¨', 'à¤¾', 'Ġà¤', 'ļ', 'à¤¾', 'à¤¹', 'à¤¿', 'à¤', 'ı', 'à¥', '¤', 'Ġà¤ķ', 'à¤¿', 'à¤¸', 'à¥Ģ', 'Ġà¤', 'Ń', 'à¥Ģ', 'Ġà¤', '¶', 'à¤', '¬', 'à¥į', 'à¤', '¦', ',', 'Ġà¤', 'µ', 'à¤¾', 'à¤ķ', 'à¥į', 'à¤¯', ',', 'Ġà¤¸', 'à¥', 'Ĥ', 'à¤¤', 'à¥į', 'à¤°', 'Ġà¤¸', 'à¥ĩ', 'Ġà¤¸', 'à¤®', 'à¥į', 'à¤', '¬', 'à¤', '¦', 'à¥į', 'à¤', '§', 'Ġà¤', 'µ', 'à¤¿', 'à¤', 'ļ', 'à¤¾', 'à¤°', 'Ġà¤', 'ı', 'à¤', 'µ', 'à¤Ĥ', 'Ġà¤', 'Ń', 'à¤¾', 'à¤', 'µ', 'à¥ĭ', 'à¤Ĥ', 'Ġà¤ķ', 'à¥ĭ', 'Ġà¤', 'ħ', 'à¤', 'ª', 'à¤¨', 'à¥ĩ', 'Ġà¤', 'ħ', 'à¤°', 'à¥į', 'à¤', 'ľ', 'à¤¿', 'à¤¤', 'Ġà¤', 'ľ', 'à¥į', 'à¤', 'ŀ', 'à¤¾', 'à¤¨', ',', 'Ġà¤', '¨', 'à¤¿', 'à¤', 'ľ', 'à¥Ģ', 'Ġà¤', 'ħ', 'à¤¨', 'à¥', 'ģ', 'à¤', 'Ń', 'à¥', 'Ĥ', 'à¤¤', 'à¤¿', 'Ġà¤¸', 'à¥ĩ', 'Ġà¤¸', 'à¤Ĥ', 'à¤', 'ľ', 'à¥ĭ', 'à¤ķ', 'à¤°', 'Ġà¤', 'ª', 'à¥į', 'à¤°', 'à¤', 'µ', 'à¤¾', 'à¤¹', 'à¤®', 'à¤¯', 'à¥Ģ', 'Ġà¤', '¶', 'à¥Ī', 'à¤²', 'à¥Ģ', 'Ġà¤ķ', 'à¥ĩ', 'Ġà¤®', 'à¤¾', 'à¤', '§', 'à¥į', 'à¤¯', 'à¤®', 'Ġà¤¸', 'à¥ĩ', 'Ġà¤', 'Ĺ', 'à¤', '¦', 'à¥į', 'à¤¯', 'à¤', 'Ń', 'à¤¾', 'à¤', '·', 'à¤¾', 'Ġà¤®', 'à¥ĩ', 'à¤Ĥ', 'Ġà¤', 'ħ', 'à¤', 'Ń', 'à¤¿', 'à¤', 'µ', 'à¥į', 'à¤¯', 'à¤ķ', 'à¥į', 'à¤¤', 'Ġà¤ķ', 'à¤°', 'à¤¨', 'à¤¾', 'Ġà¤', 'ħ', 'à¤¨', 'à¥', 'ģ', 'à¤', 'ļ', 'à¥į', 'à¤', 'Ľ', 'à¥ĩ', 'à¤', '¦', 'Ġà¤ķ', 'à¤¹', 'à¤²', 'à¤¾', 'à¤¤', 'à¤¾', 'Ġà¤¹', 'à¥Ī', 'à¥', '¤', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "ids = TOKENIZERS[\"tiny_en\"](hi_text)['input_ids']\n",
    "tokens = TOKENIZERS[\"tiny_en\"].convert_ids_to_tokens(ids)\n",
    "print(tokens, end=\"\\n\\n\")\n",
    "\n",
    "ids = TOKENIZERS[\"tiny\"](hi_text)['input_ids']\n",
    "tokens = TOKENIZERS[\"tiny\"].convert_ids_to_tokens(ids)\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
